---
title: "Statistical Inference"
subtitle: "CRM Lecture 3"
author: "Charles Lanfear"
date: "25 Oct 2022<br>Updated: `r gsub(' 0', ' ', format(Sys.Date(), format='%d %b %Y'))`"
output:
  xaringan::moon_reader:
    css: "../assets/cam-css.css"
    lib_dir: libs
    nature:
      highlightStyle: tomorrow-night-bright
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "../assets/cam_macros.js"
      titleSlideClass: ["center","top"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning = FALSE, message = FALSE, dev = "svg", fig.height = 5, dev.args=list(bg="transparent"))
standardize <- function(x){
  return((x-mean(x))/sd(x))
}
library(plotly)
library(tidyverse)
library(ggpmisc)
set.seed(7)
communities_orig <- read_csv("https://clanfear.github.io/ioc_iqa/_data/communities.csv") |>
  mutate(control = 0.5*standardize(crime_rate) - 0.1*standardize(pop_density) + rnorm(n()))
communities <- communities_orig |>
  janitor::clean_names(case = "title")
```


# Today

Regression

Estimation


---
class: inverse

# Regression

---

# Correlations

Correlation ($R$) summarizes relationship between continuous variables

--

$R$ values:

* Range from -1 to 1
* 0 indicates variables are *unrelated*
* Can be squared to obtain **coefficient of determination**: $R^2$

--

$R^2$ indicates the *proportion* of variation in one variable explained by another

--

$R^2$ values:

* Range from 0 to 1
* 0 indicates unrelated
* 1 indicates perfect relationship

--

.text-center[
*A correlation of 0.5 between $x$ and $y$ indicates 25% of the variation in $y$ is explained by $x$ (and vice versa)*
]

---

# Visualizing Correlations

```{r}
make_sample <- function(r, n){
  sig <- matrix(c(1,r,r,1),2,2)
  dat <- setNames(data.frame(MASS::mvrnorm(n=n, c(0,0), sig, empirical=TRUE)), c("x", "y"))
  return(dat)
  }
r_seq <- round(c(seq(0, 0.9, by = .15), 0.95, 1),2)

r_df <- map_dfr(setNames(r_seq, r_seq), ~ make_sample(., 100), .id = "R")
r_df$R <- factor(r_df$R, levels = r_seq)
ggplot(r_df, aes(x = x, y =y)) + 
  geom_point() + 
  facet_wrap(~R) + 
  theme_void(base_size = 16) +
  theme(panel.background = element_rect(fill='transparent', color=NA), 
    plot.background = element_rect(fill='transparent', color=NA))

```


---

# Last Week's Correlation

```{r, fig.height = 3}
communities %>%
  ggplot(aes(x = `Pop Density`, y = `Crime Rate`)) + 
  geom_point(alpha = 0.5) +
  xlab("Population Density") +
  stat_correlation() +
  theme_minimal(base_size = 16)
```

This is a strong relationship: About 66% of the variation is explained

--

.pull-left[
But what is the *substantive* interpretation?
]

--

.pull-right[
If pop density is 20, what do we expect crime to be?
]

--

text-center[
*What we want is a **model** to use pop density to predict crime*
]

---

# Regression

Regression is a method of *fitting lines* to data

--

Regression is just a way to summarize a relationship between variables using a line

* e.g., predicting $Y$ using $X$

--

* The variable being predicted is the **dependent variable**

* The variables used for prediction are **independent** or **explanatory** variables, sometimes just called **covariates**

--

The regression line describes the **conditional expectation** of the dependent variable

* e.g., the expected value of $Y$ for different values of $X$

---

# A Regression Line

```{r}
communities %>%
  ggplot(aes(x = `Pop Density`, y = `Crime Rate`)) + 
  geom_point(alpha = 0.5) +
  xlab("Population Density") +
  geom_smooth(method = "lm", se = FALSE, formula = "y ~ x") +
  theme_minimal(base_size = 16)
```

---

# Model Fitting

The line generated by our regression is called the **fit** of the **model**

--

By **fitting** the model, we choose the shape of a line (e.g., a straight one) and use maths to fit that shape to our data as best as possible

--

Our model here is a straight line fit using the **ordinary least squares** (OLS) method

--

OLS draws a line through the data that minimizes the *sum of squared errors*

* i.e., it draws a line through the data where the sum of squared distances between the line and each data point's values of $Y$

---

# Sum of Squares

```{r}
form <- y ~ x
communities %>%
  ggplot(aes(x = `Pop Density`, y = `Crime Rate`)) + 
  geom_point(alpha = 0.5) +
  xlab("Population Density") +
  stat_poly_line(formula = form, se = FALSE) +
  stat_fit_deviations(formula = form, color = "red") +
  theme_minimal(base_size = 16)
```

---

# Regression Equation

Regression lines are defined **parameters**: the **intercept** and **slope(s)** .

--

Regression line equations look like this:


$$y_i = a + bx_i + e_i$$

--

$a$ is the **intercept**

* The expected value of $y$ when $x = 0$

--

$b$ is a **coefficient**

* The amount the expected value of $y$ rises for every 1 unit increase in $x$

--

$e_i$ is the **error**

* The difference between prediction ( $a + bx$ ) and *each value* of $y$
* The square of this is what OLS is trying to minimize

---

# Regression Equation 

```{r}
form <- y ~ x
communities %>%
  ggplot(aes(x = `Pop Density`, y = `Crime Rate`)) + 
  geom_point(alpha = 0.5) +
  xlab("Population Density") +
  stat_poly_line(formula = form, se = FALSE) +
  stat_poly_eq(formula = form, mapping = use_label("eq"), eq.x.rhs = "PopDensity", eq.with.lhs= "CrimeRate~`=`~") +
  theme_minimal(base_size = 16)
```

---

# Predictions

You can think of the regression line as being a set of predictions of $Y$ for every value of $X$

--

The regression formula—the intercept and coefficient(s)—tells us what those predictions are

--

$$CrimeRate = -20.8 + 3.09 PopDensity$$

--

If $PopDensity$ is 20, then we expect $CrimeRate$ to be:

$$41 = -20.8 + 3.09*20$$

--

Put another way, we expect $CrimeRate$ to be 3.09 higher for every additional unit of $PopDensity$

--

.text-center[
*That's enough for now—we'll see more next week*
]

---
class: inverse

# Inferential Statistics

---

# Inferential Statistics


Used to help figure out if a pattern in the sample is likely to also exist in the population

Using samples to learn something about populations

Typically use **statistical tests**

Sometimes parts of **models**


---
class: inverse

# Estimation

---

Estimand

Estimator

Estimate

Estimate = Estimand + Bias + Noise

---

# Theoretical Distributions


---

# Data Generating Process

---

Hypothesis Testing

---

# Null-Hypothesis Significance Testing

Null hypothesis: There is no relationship or effect
Alternative: There is

Use data to test

We either reject or fail to reject the null; we do not *accept* the null

In reality, things are pretty always at least *a little related somehow*

---

# Frequentism

Based on logic of random sampling and experiments

Sample infinitely, and results follow a known distribution

Importance of knowing how sampling was done comes in here


---

# Coin Flipping

In theory, a normal coin is equally likely to land on heads or tails

If we flipped it ten times and got 7 heads, would we think it is fair?

What about 9 heads?


---

# Binomial Distribution

If we flipped a fair coin *infinitely*, we'd be this likely to see each count of heads:

```{r}
draws <- tibble(Heads = 0:10,
                Probability = map_dbl(Heads, ~dbinom(., 10, 0.5)),
                `95%` = Probability > 0.025 & Probability < 0.975)

ggplot(draws, aes(x = Heads, y = Probability, fill = `95%`)) + 
  geom_col() + 
  scale_x_continuous(breaks = 0:10) +
  theme_minimal() +
  theme(legend.position = "none")
```

At least 95% of the time, we'd expect to see between 2 and 8 heads.

---

```{r}
draws <- tibble(Heads = 0:10,
                Probability = map_dbl(Heads, ~dbinom(., 10, 0.5)),
                Insignificant = Probability <= 0.025 | Probability >= 0.975)

ggplot(draws, aes(x = Heads, y = Probability, fill = Insignificant)) + 
  geom_col() + 
  scale_x_continuous(breaks = 0:10)
```

---

# Sampling Animation


```{r}
library(tidyverse)
population <- rnorm(100000, 30, 10)
mean(population)

sample_means <- map_dbl(1:5000, ~mean(sample(population, 10)))

plot(density(sample_means))
```

---

# p-values

The probability of observing our result if the null hypothesis were true.

Not the probability our result is false; it either is, or it isn't

All a p-value tells us is whether the particular theoretical distribution we are considering is likely to have generated the data we observe

---

# Confidence Intervals

Confidence intervals are closely related to p-values

Intuitively, we can think of them as a range of reasonably likely values for the population mean, given our observed sample mean

If this overlaps zero, then a population mean of zero is consistent with our data

When your p-value is significant, your confidence interval will not overlap zero.

When your p-value is not significant, your confidence interval overlaps zero.

$$CI = \bar{X}\pm z_\alpha* S(X)$$

---

# Cross-Tab

So we draw a sample and see this association in the table


XXXX


We want to know if there's a real relationship in the population

We can't really know that for sure, so we rephrase the question

How likely is it that we'd see this result if there was really no relationship in the population?

We can know this for sure using theoretical distributions!

Put another way, use use sample data to rule out certain theoretical distributions

* When we reject a hypothesis, we reject the associated theoretical distribution as unlikely to have generated our data


---

# Theoretical Distributions

---

Significance testing is basically just comparing observed distributions to theoretical ones

---

# Means

When we want to infer something about the population from a sample, we often focus on some kind of mean

This is because means have a useful sampling property:

No matter what data generating process produced the observed data, the distribution of the 
*means* of all samples will follow a normal distribution<sup>1</sup>

.footnote[[1] Assuming the real distribution has a finite mean and variance]

$$
\bar{X} = \hat{\mu} \sim N(\mu, \frac{\sigma}{\sqrt{n}})
$$

---

# Central Limit Theorem


```{r}
pois_draws <- runif(100000, 0, 10)
hist(pois_draws)

mean_draws <- map_dbl(1:10000, ~mean(sample(pois_draws, 50)))
hist(mean_draws, breaks = 50)
mean(pois_draws)
```


---

# Law of Large Numbers


---

# Choosing Tests

Sometimes you get a chart 


TAKE EXACT SAME DISTRIBUTION EXAMPLES FROM BEFORE AND MAKE TESTS

---

# Cross-Tab


---

# Difference in Means


---

Turns out almost everything is a difference in means test

---

# Correlation Significance

There's a real correlation in population

We draw a sample and measure a correlation

If we sampled infinitely, we'd find the sample correlations are normally distributed around the population correlation