<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Causality and Theory Testing</title>
    <meta charset="utf-8" />
    <meta name="author" content="Charles Lanfear" />
    <script src="libs/header-attrs-2.17/header-attrs.js"></script>
    <link rel="stylesheet" href="../assets/cam-css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, top, title-slide

.title[
# Causality and Theory Testing
]
.subtitle[
## CRM Lecture 3
]
.author[
### Charles Lanfear
]
.date[
### 1 Nov 2022<br>Updated: 28 Oct 2022
]

---





# Today

* What is causality anyway?

* Causal graphs

   * Types of paths
   * An example

* Theory testing

   * Turning theories into graphs
   * Turning graphs into estimators



---

# Causality (in Brief)


`\(X\)` causes `\(Y\)` if changing *only* `\(X\)` would change `\(Y\)`

--

&amp;zwj;This is quite broad:

* `\(X\)` is typically not the only cause

   * e.g., `\(X\)` and `\(Z\)` can both be causes of `\(Y\)`

--

* `\(X\)` may cause `\(Y\)` for multiple reasons

   * e.g., `\(X\)` may cause `\(Z\)` which causes `\(Y\)` ( `\(Z\)` is a *mechanism*)

--

* `\(X\)` may only cause `\(Y\)` in certain contexts

   * e.e., `\(X\)` may cause `\(Y\)` only when `\(Z\)` is present

--

.text-center[
*All that matters for the relationship to be causal is that changing the distribution of `\(X\)` would result in a different distribution of `\(Y\)`*
]

--

We call the process of estimating a causal effect **identification**

---

# Theory and Causes


All scientific theories are causal: *They explain **why** things happen*

--

In criminology:

* "Increasing the perceived risk of being caught reduces the likelihood of a person committing a particular crime"
* "Increasing a person's self-control reduces their involvement in all crime"
* "Learning violent definitions of situations increases a person's likelihood of using violence"
* "Strong social institutions in neighbourhoods reduce crime."
* "Reducing interactions between potential offenders and targets reduces crime in places."

--

These are all testable, causal propositions about how the world works

--

Theory testing is **causal inference**: Drawing *causal conclusions* from data

* Doing this convincingly requires identifying causal effects



---

# The Problem

Let's say we want to know if we can reduce crime by clearing vacant lots

* We want to identify the effect of lot clearing on crime

--

But we only see *one outcome* per lot:&lt;sup&gt;1&lt;/sup&gt;

* If we clear a lot, we don't see what happens if we had not.
* If we *don't* clear it, we don't see what happens if we had.

.footnote[[1] This is often called the "fundamental problem of causal inference"]

--

We call whichever outcome we don't see a **counterfactual**—it is counter to the fact of what actually happened

--

Causal effects are *differences* between these **potential outcomes**:

1. What actually happened, which we see (factual)
2. An alternative, which we do not observe (counterfactual)

--

.text-center[
*How do we calculate a difference when we only see one outcome?*
]

---

# Between Comparison

.pull-left[

We could compare two lots:

* **One not cleared (T=0)**
* .strong-blue[One cleared (T=1)]


And use the difference in crime (bT) as a causal effect

]
.pull-right[
![:width 80%](img/lots.svg)
]

--

&amp;nbsp;

*But what if the difference is because the lots are different?*

--

* We could compare only lots that look *identical*

--

*But, if those lots are identical, why did one get cleared and not the other?*

* We might have missed an important difference
* We wouldn't be sure unless we knew *why* each lots get cleared

---

# Within Comparison

.pull-left[

We could compare the same lot:

* **Before clearing (T=0)**
* .strong-blue[After (T=1)]

And use the before-and-after crime difference (bT) as a causal effect

]
.pull-right[
![:width 80%](img/lot.svg)
]

--

&amp;nbsp;

*But what if the crime change was going to happen anyway?*

* The clearing simply coincided with the crime change
* Worse, maybe the change in crime *caused the clearing*

--

*Or, what if the lot was cleared **because** it would make a difference here?*

* What if there would be no difference for *other* lots?

---

# Potential Outcomes

.pull-left[
All of these problems are really one problem:

*Differences in **potential outcomes** between cleared and uncleared lots*
]

.pull-right[
![:width 80%](img/potential_lots.svg)
]

--

These problems are resolved if the potential outcomes for cleared and uncleared lots are *the same on average* (unrelated to the treatment)

The average difference in outcomess then provides an unbiased estimate of the causal effect (the estimand)

--


.text-center[
*How do we make sure the cleared and uncleared lots are the same?*
]


---

# Randomization

*Randomized controlled trials do this*

* Take a relatively large number of lots
--

* Randomly assign treatment: clear some lots but not others
--

* On average, cleared and uncleared will be similar *in every way except for clearing*

--

Randomization makes treatment **independent** of potential outcomes

* Clearing is equally likely no matter how much a lot would benefit

--

If we randomly assign a treatment, we can estimate a causal effect as the average difference in outcomes between treated groups

* We call this the **average treatment effect**
* You can calculate this with a *cross-tab*

--

.text-center[
*What if we're interested in things we can't randomly assign?*
]

---

# Back to the DGP

When we can't **randomize**, we need to **theorize**

--

We need to figure out the **data generating process** for our treatment and outcome

* What makes some units get treated?
* What makes treatment more (or less) effective for some units?

--

This requires making **assumptions**, which are part of your **model**

* `\(Z\)` is unrelated to `\(X\)`
* `\(B\)` only causes `\(Y\)` through `\(C\)`

--

.text-center[
*If our model is correct, it tells us exactly how to identify the causal effect we want*
]

--

.text-center[
*So how do we make the model?*
]

---
class: inverse

# Directed Acyclic Graphs

&amp;nbsp;

![:width 70%](img/dags-do-you-like-dags.gif)

---

# Causal Graphs

.pull-left[
Classic statistics is mathy, using things like this:

`$$Y = f(X,Z)$$`
... to say `\(Y\)` is caused by a combination of `\(X\)` and `\(Z\)`

]

--

.pull-right[

We're going to work with graphs instead:

![](slides_causality-testing-theories_files/figure-html/dag-1-1.svg)&lt;!-- --&gt;

]

--

Directed acyclic graphs have pretty simple rules:

* Arrows between variables point from causes to outcomes (directed)

--

* No arrow means no (direct) causal relationship

--

* Something can't cause itself... *even through other things* (acyclic)

--

.text-center[
*Causal graphs are a way to write out a model that tells you how to identify effects*
]

---

# Paths

.pull-left[

A "path" is any set of arrows linking two variables together

There are three paths from X to Y:

* `\(X \rightarrow Y\)`
* `\(X \leftarrow Z \rightarrow Y\)`
* `\(X \rightarrow W \leftarrow Y\)`

]

.pull-right[
![](slides_causality-testing-theories_files/figure-html/dag-2-1.svg)&lt;!-- --&gt;
]

--

Let's say we want to know the effect of `\(X\)` on `\(Y\)`

* Paths where all arrows point from `\(X\)` to `\(Y\)` are called **front doors**
* Paths where arrows point to both `\(X\)` to `\(Y\)` are **back doors**
* Paths where arrows point *at* each other are **colliders**

--

We estimate effects *through front doors* by closing *all back doors*



---

# Closing Back Doors

Variables that open back doors are called **confounders**

* Confounders cause *dependence between treatment and potential outcomes*

--

We close doors by **adjusting** or **controlling** or **holding constant** variables along the path in statistical models&lt;sup&gt;1&lt;/sup&gt;

.footnote[[1] We'll see this in a bit]

--

.pull-left[

Adjusting for Z closes the back door

The only remaining path is the front door we're interested in

The effect of `\(X\)` on `\(Y\)` is identified


]

.pull-right[

![](slides_causality-testing-theories_files/figure-html/dag-3-1.svg)&lt;!-- --&gt;
]

---

# More Front Doors

Very often your model has multiple front doors

--

You don't adjust for these—they're ways `\(X\)` impacts `\(Y\)`!

* We call these **mechanisms** or **mediators**

--

.pull-left[

We've still adjusted for `\(Z\)`

But there are *two* front doors

* `\(X \rightarrow Y\)`
* `\(X \rightarrow A \rightarrow Y\)`

The effect of `\(X\)` on `\(Y\)` is identified!
]

.pull-right[

![](slides_causality-testing-theories_files/figure-html/dag-4-1.svg)&lt;!-- --&gt;
]

--

&amp;nbsp;

The *total* estimated effect of `\(X\)` on `\(Y\)` will be the sum of all paths leading between them.

---

# Colliders

Colliders are the *opposite* of back door paths: They *block* associations from going down their path

--

If you control for a collider on the path between `\(X\)` and `\(Y\)`, it opens the association back up and *introduces bias*

--

.pull-left[

Adjusting for Z closes the back door

But here we've adjusted for `\(W\)` too

This introduces bias!


]

.pull-right[

![](slides_causality-testing-theories_files/figure-html/dag-5-1.svg)&lt;!-- --&gt;
]

--

Controlling for a collider is sometimes called **selection bias** or **conditioning on a post-treatment variable**.



---

# Unobservabed Variables

A DAG *must* accurately represent what we think the data generating process is to be useful

--

Sometimes we know a variable is important but we can't measure it


--

.pull-left[

Unobserved variables are often indicated with circles

Because they're unobserved, we can't adjust for them *directly*

Here we can still block the path through `\(W\)` by adjusting for `\(Z\)`
]

.pull-right[

![](slides_causality-testing-theories_files/figure-html/dag-6-1.svg)&lt;!-- --&gt;
]

--

Most of the time we can't observe *everything* that matters, but rigorous research will acknowledge this

--

.text-center[
*This is all very abstract, so let's put them to work*
]

---
class:

# DAGs at Work

&amp;nbsp;

&amp;nbsp;

![:width 80%](img/dag_work.jpg)

---
class: inverse
# Measuring Bias in Policing

&amp;nbsp;

![:width 80%](img/klm.png)

---
# Use of Force


&amp;zwj;Question: *Do US police use more force against black civilians?*

--

Imagine you sample police encounters **identical** except for race

*But* suppose bias leads police to:

 * Stop white civilians only for crimes
 * Stop black civilians with or without crime

--

Then, discard data on anyone police observed but *did not stop*.

You are now comparing use of force between two groups:
   
* White people committing crime
* Black people committing no crime *or* committing a crime

--

If use of force were the **same**, *we'd have a serious problem*

--

*This is what police data actually show: we only see the stopped people!*


---

# What We Want

&amp;nbsp;

![](slides_causality-testing-theories_files/figure-html/klm-1-1.svg)&lt;!-- --&gt;

&amp;nbsp;

&amp;zwj;Question: How does race impact use of force?

--

We could compare how often force is used on white vs. black subjects

---
# The Problem

&amp;nbsp;

![](slides_causality-testing-theories_files/figure-html/klm-2-1.svg)&lt;!-- --&gt;

&amp;nbsp;

&amp;zwj;Problem: Race impacts likelihood of stop, but *we never see unstopped people*

* We've conditioned on *being stopped*—a mediator!
* We've blocked part of our front door!

--

If police stop black subjects for more minor things, then an equal proportions of use of force *actually indicates bias*

---
# It Gets Worse

&amp;nbsp;

![](slides_causality-testing-theories_files/figure-html/klm-3-1.svg)&lt;!-- --&gt;

&amp;nbsp;

More problems

* Racial composition varies by neighborhood
* Police deployments and strategy vary by neighborhood
* Reasonable suspicion predicts both stops and use of force--and can't be observed


---
# Consequences

* Detecting bias in the decision to stop is difficult

--

* If any stop bias exists, it is difficult to measure bias in...

   * Arrests
   * Use of force
   * Frisks
   * Shootings

--

* Raw numbers can easily show *opposite patterns* from underlying reality.

* Studies showing no bias—or anti-white bias—get *lots* of media, social media, and political traction.

---
# Theory Testing

&amp;nbsp;

![:width 90%](img/bursik_grasmick.svg)

.footnote[Source: Adapted from Bursik &amp; Grasmick 1993]

---

# Recursive Models

Criminological theories tell us how to construct DAGs

--

Often this is fairly explicit, such as when authors provide a causal diagram... *and it already obeys DAG rules*

--

&amp;nbsp;

![:width 70%](img/matsueda_1982.png)

.footnote[Source: [Matsueda (1982)](https://doi.org/10.2307/2095194)]


---

# Nonrecursive Models

This is less clear when the causal diagram provided doesn't conform to DAG rules

--

![:width 70%](img/lanfear_2018.png)

--

Converting a model with **cycles** into an **acyclic** one can take some thought

--

*All causal relationships really are acyclic though*

---

# Broken Windows DAG

Usually the trick to converting a cyclic graph to an acyclic one is realizing effects *occur over **time***

&amp;nbsp;

![](slides_causality-testing-theories_files/figure-html/bwt-1.svg)&lt;!-- --&gt;

--

Another way is through **intervening** or **cross-level** mechanisms

* Many macro theories are circular without individual-level mechanisms

---

# No Graphical Model

Making a DAG is least clear when the authors provide no graphical representation of the model at all&lt;sup&gt;1&lt;/sup&gt;

.footnote[[1] Or when you're developing something new yourself!]

--

The process:

* Record all the core variables they use

   * e.g., self control

--

* Make note of causal statements

   * e.g., "self control **influences** crime by..."
   * Make these into arrows!

--

* Try to reconcile circular paths

   * Consider temporal order
   * Consider cross-level mechanisms

---
class: inverse

# From DAG to Estimator

---

# Estimating Causal Effects

Once you have a DAG, it is remarkably easy to proceed to an estimator

--

The process:

1. Figure out what you can and cannot measure

--

2. If your measures let you close all back doors...

   * Add any back door variables as controls
   * Leave out any colliders or mechanisms
   * Estimate the causal effect

--

3. If you can't close all back doors...

   * Close what you can but settle without identification
      * Note threats to identification
      * Perform sensitivity tests
      * Estimate a non-causal relationship
   * Give up
   * *Get clever*

---
# Controlling for Variables


---

# Regression Again

So far we've just fit a simple straight line using one variable predicting another

.pull-left[

------------------------
    Term       Estimate 
------------- ----------
 Pop Density     3.09   

 (Intercept)    -20.78  
------------------------


]
.pull-right[

![](slides_causality-testing-theories_files/figure-html/unnamed-chunk-10-1.svg)&lt;!-- --&gt;

]


---

# Multiple Predictors

When we add predictors, we are **adjusting** or **controlling** for them to identify the effect we're interested in


------------------------
    Term       Estimate 
------------- ----------
    Urban       -3.39   

 Pop Density     3.36   

 (Intercept)    -23.04  
------------------------

New interpretation:

* "The association between density and crime, adjusting for the type of area*"
* "The average crime rate difference for urban areas, compared to rural ones, adjusting for population density"

---

# How it Works



![:width 80%](img/control_anim.gif)

---

# Causality without Correlation

No correlation between speed of a car and how far gas pedal is pushed when going up and down hills







---

# More Variables

&amp;nbsp;

---

# Causality and QUalitative Data

Sometimes quantitative data is not up to the task of identifying an effect of interest


McElreath gender, citations, NAS, quality DAG
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="../assets/cam_macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "tomorrow-night-bright",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
