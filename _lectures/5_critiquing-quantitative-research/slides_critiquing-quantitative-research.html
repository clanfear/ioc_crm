<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Critiquing Quantitative Research</title>
    <meta charset="utf-8" />
    <meta name="author" content="Charles Lanfear" />
    <script src="libs/header-attrs-2.17/header-attrs.js"></script>
    <link rel="stylesheet" href="../assets/cam-css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, top, title-slide

.title[
# Critiquing Quantitative Research
]
.subtitle[
## CRM Lecture 5
]
.author[
### Charles Lanfear
]
.date[
### 8 Nov 2022<br>Updated: 07 Nov 2022
]

---





# Dual Purposes

Being a critical consumer of research

* This is focus for today

--

Being a critical producer of research

* Turning critical consumption on yourself

--

Recognizing **rigour**

* Adherence to best practices
* Self-critical research

--

.text-center[
*Rigourous research is doing the best with what is available and being honest about the process, limitations, and conclusions*
]


---
class: inverse

# Validity

&amp;nbsp;

&amp;nbsp;



---

# Types of Validity

Internal validity

* **Identification**
* Congruence between theory and method

--

Construct Validity

* **Measurement**
* Congruence between conceptualization and operationalization

--

External validity

* **Generalizability**
* Sample-to-population or inter-population inference

--

Farrington separates out:

* Statistical conclusion validity
* Descriptive validity


---

# Internal Validity

**Internal validity** is the degree to which the quantity of interest is **identified**

--

* Causal: Are we confident the observed effect is due to the treatment?

* Non-causal: Are we confident the observed quantity is close to the quantity in the population?



--

Things to look out for:

* Well-explained **data generating process** (week 4)
* Sufficient **statistical power**
* Plausible **effect sizes**

---

# Data Generating Process

The key to internal validity is *accurately modeling the relevant parts of the **data generating process***

--

When reading research:

* Draw the DAG for papers you read closely
* Evaluate it carefully
* If it is important, run it by a colleague or mentor

--

When conducting research:

* **Always draw the DAG**
* Include it in your paper unless it is *very simple*
* Always have a colleague or mentor look at it

--

.text-center[
*Authors should make a convincing case for their model of the DGP*
]

---

# Statistical Power

The ability to *detect a meaningful effect* is a requirement for valid results


--


.pull-left[

&amp;zwnj;Power:

* &amp;uarr; with sample size

* &amp;uarr; with strength of effect

* &amp;darr; with confidence level

* &amp;darr; with noise
]
.pull-right[
![:width 90%](img/power-analysis.png)
]

--

Must differentiate between **precise nulls** and inconclusive evidence

* A precise near-zero estimate is evidence for *absence of an effect*
* A low precision insignificant estimate is not evidence for *no effect*

.text-center[
*Be wary of claims from underpowered studies&lt;br&gt;(e.g., with large confidence intervals)*
]

---

# Effect Sizes

Rigorous research will report and interpret **effect sizes**

* e.g., "Cleared lots displayed 30% less crime than uncleared lots"
* Larger effects are more **substantively significant**

--

Be suspicious of *very strong effects*, especially for minor treatments

* e.g., "rates of theft were 100% higher under the litter treatment"

--

Be suspicious of *too much explained variation*

* e.g., "eviction explains the entire effect of socioeconomic disadvantage on crime rates"

--

.text-center[
*The more variation one factor explains, the less others can explain*

*In complex data generating processes, it is unlikely that changing a single factor will cause large differences in the outcome*
]

---

# Estimation Complications

Thus far we've left out most technical details of **estimation** (i.e., statistical validity)

--

.pull-left[
For example:

* Functional forms

]
.pull-right[
![](slides_critiquing-quantitative-research_files/figure-html/unnamed-chunk-1-1.svg)&lt;!-- --&gt;
]

---
count:false

# Estimation Complications

Thus far we've left out most technical details of **estimation** (i.e., statistical validity)

.pull-left[
For example:

* Functional forms

* Error distributions

]
.pull-right[
![](img/poisson-residuals.png)

[Source](https://www.theanalysisfactor.com/poisson-or-negative-binomial-using-count-model-diagnostics-to-select-a-model/)
]


---
count:false

# Estimation Complications

Thus far we've left out most technical details of **estimation** (i.e., statistical validity)

.pull-left[
For example:

* Functional forms

* Error distributions

* Missing data

]
.pull-right[
![](img/multiple-imputation.png)

[Source](https://hdsr.mitpress.mit.edu/pub/4tx7h11w/release/2)
]


--

.text-center[
*These are all important*
]

--

.text-center[
*But they don't matter if the identification strategy is wrong!*
]


---

# Construct Validity

.text-center[
*Do your measured variables capture the concepts in the DGP that you're interested in?*
]

![](slides_critiquing-quantitative-research_files/figure-html/dag-1-1.svg)&lt;!-- --&gt;

--

Common problems:

* Bad proxies / poor convergent validity

   * Measures that don't capture the unobservable of interest

--

* Poor discriminant validity

   * Measures that are indistinguishable from competing constructs
   
--

* Measurement error and low reliability

   * Measures that contain excess random (or non-random) variation

--

* **Inconsistent treatments**

---

# Consistency

**Consistency** is a necessary assumption of identifying causal effects

--

* There is only **one** type of treatment for any measured level

   * e.g., clearing a vacant lot as paving it vs. creating a park
   * **Compound treatment**: Multiple treatments inadvertently analyzed as if a single treatment

--

* An experimentally-assigned treatment is identical to the naturally-assigned one of interest

   * e.g., litter placed on a sidewalk is equivalent to "natural" litter

--

Consistency is a *major threat* to experimental research

* Implausible or unnatural lab and field experiment treatments
* Some treatments are inseparable from other conditions

--

.text-center[
*If a treatment sounds contrived, it is probably inconsistent*
]

---

# External Validity

*Would the results look similar in a new context?*

--

Threats to external validity:

* Non-representative samples

   * Treatment effect may only exist in this sample
   * Treatment effects may *differ* across populations

--

* Inconsistent treatments

   * Treatment effect may not exist when treatment is assigned naturally
   * "High fidelity" treatments may only exist in less-controlled contexts (e.g., field vs. lab) that hinder identification

--

* Context-dependent results

   * Same units may behave differently in other contexts
   * Underlying data generating process can *change*


---
class: inverse

# Quantitative Skepticism

&amp;nbsp;

&amp;nbsp;

![:width 40%](img/skepticism.gif)

---

# Science is Cumulative

.text-center[
*Most new novel findings are wrong*
]

--

When should you trust a finding?

--

When it is **replicated**

* The more often something is found, the more you should be convinced

--

When it is **triangulated**

* The more something is found using different data sets and analysis approaches, the more you should be convinced

* Corollary: If something is only found using the same methods or with the same kind of data, you should be more skeptical

--

It is **compatible** with other trustworthy findings

* If the finding being true would overturn a well-established literature, you should be more skeptical

---

# Goodhart's Law

&gt; When a measure becomes a target, it ceases to be a good measure.&lt;sup&gt;1&lt;/sup&gt;

Any quantitative measure will be *gamed* if it benefits individuals

.footnote[
[1] Originally [Goodhart (1975)](https://link.springer.com/chapter/10.1007/978-1-349-17295-5_4), but restated by [Strathern (1997)](https://doi.org/10.1002/%28SICI%291234-981X%28199707%295:3&lt;305::AID-EURO184&gt;3.0.CO;2-4)&lt;br&gt;
Goodhart's law is sometimes a case of *bad proxies*
]

--

Goodhardt's law can produce **perverse incentives**: Incentives that encourage *the opposite* of what was intended

* *The Cobra Effect* during Crown rule in India

* Police clearance rates

* *Academic publishing!*

---

# Peer Review

.pull-left[
A lot of great research is a struggle to get published

A lot of bad research gets published in high profile outlets
]
.pull-right[
![](img/retracted.png)
]

&amp;nbsp;

Being in a particular journal is a weak signal of quality for individual papers

* Publication process is highly random

--

Despite this, some journals are consistently very good&lt;sup&gt;1&lt;/sup&gt;

* Good editors and editorial teams
* Ability to effectively solicit good reviewers

.footnote[[1] This is area specificâ€”ask your mentors!]



---

# Secondhand Accounts

Authors commonly misstate the findings of cited papers

* Citing based on statements found in other articles
* Citing based on abstract, or worse, title
* Reading but misunderstanding

--

If authors make past research sound crazy, they're typically...

* Purposefully misrepresenting it
* Misunderstanding it

--

Don't contribute to this yourself:

* Don't cite things you didn't read
* Don't cite things you don't understand
   * Get help to understand important but challenging research

--

.text-center[
*Science is mostly reading other people's work*
]

---
class:inverse

# Reading the Literature

&amp;nbsp;

![:width 80%](img/spiderman.jpg)

.footnote[
Source: [Road Trip with Raj @ unsplash](https://unsplash.com/photos/o4c2zoVhjSw)
]

---

# Review Papers

Reviews are a good starting point for new scholars

.pull-left-60[
* Summary reviews

   * Focus: Summarizing area
   * Audience: Newcomers

]

.pull-right-40[
![](img/wilcox-cullen.png)
]

---
count: false

# Review Papers

Reviews are a good starting point for new scholars

.pull-left-60[
* Summary reviews

   * Focus: Summarizing area
   * Audience: Newcomers

* Systematic reviews

   * Focus: Assessing evidence
   * Audience: 
      * Decisionmakers
      * Area researchers
   * Search process
]

.pull-right-40[
![](img/systematic.png)
]

---
count: false

# Review Papers

Reviews are a good starting point for new scholars

.pull-left-60[
* Summary reviews

   * Focus: Summarizing area
   * Audience: Newcomers

* Systematic reviews

   * Focus: Assessing evidence
   * Audience: 
      * Decisionmakers
      * Area researchers
   * Search process

* Critical reviews

   * Focus: Gaps / challenges
   * Audience: Area researchers
]

.pull-right-40[
![](img/arc.png)
]

---

# Final Thoughts


Signals of rigorous science:

* Open and unambiguous

   * Everything that was done is reported in detail
   * Decisions are noted and justified
   * **Reproducibility**
   * **Pre-registration**

--

* Careful

   * Tests alternative explanations
   * Uses sensitivity or robustness tests

--

* Honest

   * Provides realistic implications
   * Describes assumptions and consequences if violated

---
class: inverse

# Wrap-Up

The quantitative half of the course is now over!

&amp;nbsp;

![](img/shawshank.png)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="../assets/cam_macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "tomorrow-night-bright",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
